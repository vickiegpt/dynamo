# LLM Router Helm Chart Enhancement Implementation
# Author: LLM Router Team
# Purpose: Support both cloud and local model deployments

# =============================================================================
# 1. Enhanced values.yaml
# =============================================================================

routerController:
  enabled: true
  image:
    repository: router-controller
    tag: latest
    pullPolicy: IfNotPresent
  
  # Configuration Strategy (choose one)
  config:
    # Strategy 1: External ConfigMap (highest flexibility)
    existingConfigMap: ""  # If set, uses existing ConfigMap instead of generating
    
    # Strategy 2: Inline custom config (medium flexibility)
    customConfig: ""  # If set, uses this YAML content directly
    
    # Strategy 3: Template-based config (structured approach)
    template:
      enabled: true  # Use templated configuration
      
      # Backend configuration
      backend:
        type: "nvidia-cloud"  # Options: nvidia-cloud, local-service, custom
        
        # For nvidia-cloud backend
        nvidia:
          apiBase: "https://integrate.api.nvidia.com"
          apiKeySecret: "llm-api-keys"
          apiKeySecretKey: "nvidia_api_key"
        
        # For local-service backend  
        local:
          apiBase: ""  # e.g., "http://vllm-frontend.dynamo.svc.cluster.local:8000/v1"
          apiKeySecret: ""  # Optional for local services
          apiKeySecretKey: ""
        
        # For custom backend
        custom:
          apiBase: ""
          apiKeySecret: ""
          apiKeySecretKey: ""
      
      # Model routing configuration
      policies:
        - name: "task_router"
          url: "http://{{ include \"llm-router.fullname\" . }}-router-server:8000/v2/models/task_router_ensemble/infer"
          llms:
            brainstorming:
              model: "meta/llama-3.1-70b-instruct"  # Cloud model name
              localModel: "Qwen/Qwen3-0.6B"         # Local model name
            chatbot:
              model: "mistralai/mixtral-8x22b-instruct-v0.1"
              localModel: "Qwen/Qwen3-0.6B"
            classification:
              model: "meta/llama-3.1-8b-instruct"
              localModel: "Qwen/Qwen3-0.6B"
            # ... other models
        
        - name: "complexity_router"
          url: "http://{{ include \"llm-router.fullname\" . }}-router-server:8000/v2/models/complexity_router_ensemble/infer"
          llms:
            creativity:
              model: "meta/llama-3.1-70b-instruct"
              localModel: "Qwen/Qwen3-0.6B"
            reasoning:
              model: "nvidia/llama-3.3-nemotron-super-49b-v1"
              localModel: "Qwen/Qwen3-0.6B"
            # ... other models

# =============================================================================
# 2. Enhanced templates/router-controller-configmap.yaml
# =============================================================================

{{- if not .Values.routerController.config.existingConfigMap }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "llm-router.fullname" . }}-router-controller-config
  labels:
    {{- include "llm-router.labels" . | nindent 4 }}
    app.kubernetes.io/component: router-controller
data:
  config.yaml: |-
{{- if .Values.routerController.config.customConfig }}
    # User-provided custom configuration
{{ .Values.routerController.config.customConfig | indent 4 }}
{{- else if .Values.routerController.config.template.enabled }}
    # Template-based configuration
    policies:
{{- range .Values.routerController.config.template.policies }}
      - name: {{ .name | quote }}
        url: {{ tpl .url $ }}
        llms:
{{- $backend := $.Values.routerController.config.template.backend }}
{{- range $taskName, $taskConfig := .llms }}
          - name: {{ $taskName | title }}
{{- if eq $backend.type "nvidia-cloud" }}
            api_base: {{ $backend.nvidia.apiBase }}
            api_key: ${NVIDIA_API_KEY}
            model: {{ $taskConfig.model }}
{{- else if eq $backend.type "local-service" }}
            api_base: {{ $backend.local.apiBase }}
{{- if $backend.local.apiKeySecret }}
            api_key: ${NVIDIA_API_KEY}
{{- else }}
            api_key: ${NVIDIA_API_KEY}  # Placeholder for local services
{{- end }}
            model: {{ $taskConfig.localModel | default $taskConfig.model }}
{{- else if eq $backend.type "custom" }}
            api_base: {{ $backend.custom.apiBase }}
            api_key: ${NVIDIA_API_KEY}
            model: {{ $taskConfig.localModel | default $taskConfig.model }}
{{- end }}
{{- end }}
{{- end }}
{{- else }}
    # Default NVIDIA Cloud configuration (backward compatibility)
    policies:
      - name: "task_router"
        url: http://{{ include "llm-router.fullname" . }}-router-server:8000/v2/models/task_router_ensemble/infer
        llms:
          - name: Brainstorming
            api_base: https://integrate.api.nvidia.com
            api_key: ${NVIDIA_API_KEY}
            model: meta/llama-3.1-70b-instruct
          # ... rest of default config
{{- end }}
{{- end }}

# =============================================================================
# 3. Enhanced templates/router-controller-deployment.yaml
# =============================================================================

# In the volumes section:
      volumes:
        - name: config-volume
          configMap:
{{- if .Values.routerController.config.existingConfigMap }}
            name: {{ .Values.routerController.config.existingConfigMap }}
{{- else }}
            name: {{ include "llm-router.fullname" . }}-router-controller-config
{{- end }}

# =============================================================================
# 4. Usage Examples
# =============================================================================

# Example 1: Dynamo Integration (Template-based)
# values-dynamo.yaml
routerController:
  config:
    template:
      enabled: true
      backend:
        type: "local-service"
        local:
          apiBase: "http://vllm-frontend-frontend.dynamo-kubernetes.svc.cluster.local:8000/v1"
          apiKeySecret: ""  # No API key needed for local services

# Example 2: Custom Configuration (Maximum flexibility)
# values-custom.yaml  
routerController:
  config:
    customConfig: |
      policies:
        - name: "task_router"
          url: http://router-server:8000/v2/models/task_router_ensemble/infer
          llms:
            - name: Brainstorming
              api_base: http://my-custom-service:8000/v1
              api_key: ${NVIDIA_API_KEY}
              model: my-custom-model

# Example 3: External ConfigMap (Advanced users)
# values-external.yaml
routerController:
  config:
    existingConfigMap: "my-router-config"

# Example 4: Default Cloud (Backward compatibility)
# values-cloud.yaml (or no values file)
# Uses default NVIDIA Cloud configuration automatically
