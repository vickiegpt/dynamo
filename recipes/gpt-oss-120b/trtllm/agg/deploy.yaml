# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: gpt-oss-agg-shm
spec:
  backendFramework: trtllm
  pvcs:
    - name: model-cache-oss-gpt120b
      create: false
  services:
    TrtllmWorker:
      componentType: main
      dynamoNamespace: gpt-oss-agg-shm
      envFromSecret: hf-token-secret
      volumeMounts:
        - name: model-cache-oss-gpt120b
          mountPoint: /root/.cache/huggingface
      sharedMemory:
        size: 80Gi
      extraPodSpec:
        tolerations:
          - key: "dedicated"
            operator: "Equal"
            value: "user-workload"
            effect: "NoSchedule"
          - key: "dedicated"
            operator: "Equal"
            value: "user-workload"
            effect: "NoExecute"
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: nvidia.com/gpu.present
                      operator: In
                      values:
                        - "true"
        mainContainer:
          args:
          - |
            export TRTLLM_ENABLE_PDL=1
            export TRT_LLM_DISABLE_LOAD_WEIGHTS_IN_PARALLEL=True
            export ENGINE_ARGS=${AGG_ENGINE_ARGS:-"/root/.cache/huggingface/gpt-oss-120b/config.yaml"}
            export MODEL_PATH=${MODEL_PATH:-"/root/.cache/huggingface/models--openai--gpt-oss-120b/snapshots/b5c939de8f754692c1647ca79fbf85e8c1e70f8a"}
            export SERVED_MODEL_NAME=${SERVED_MODEL_NAME:-"openai/gpt-oss-120b"}
            trap 'echo Cleaning up...; kill 0' EXIT
            python3 -m dynamo.frontend --router-mode round-robin --http-port 8000 &
            python3 -m dynamo.trtllm \
              --model-path "$MODEL_PATH" \
              --served-model-name "$SERVED_MODEL_NAME" \
              --extra-engine-args "$ENGINE_ARGS" \
              --max-num-tokens 20000 \
              --max-batch-size 640 \
              --free-gpu-memory-fraction 0.9
          command:
          - /bin/sh
          - -c
          image: my-registry/vllm-runtime:my-tag
          workingDir: /workspace/components/backends/trtllm
      replicas: 1
      resources:
        limits:
          gpu: "4"
        requests:
          gpu: "4"