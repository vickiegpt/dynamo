# Dynamo docs publish workflow
# - Automatically runs after successful generate-docs workflow on main
# - Pushes to main: publish to S3 under 'latest' only
# - Version tags: publish to S3 under the parsed version (vX.Y[.Z][suffix]);
#   does NOT update 'latest'.
# - Akamai: always flushes cache for the target path after publish
# - Repo variable required: PUBLISH_S3_TARGET_PATH (prefix under s3://brightspot-assets-prod/developer/docs)
# - Secrets required: AWS credentials (role or keys), AKAMAI_* EdgeGrid creds
# - To skip publishing 'latest' on main, include a line '/not-latest' in the commit message
name: Publish Documentation to S3

on:
  workflow_run:
    workflows: ["Generate Documentation"]
    types:
      - completed
    branches:
      - main
  push:
    tags:
      - '*'
  workflow_dispatch:
    inputs:
      run_id:
        description: 'Optional: Specific generate-docs run ID to use for artifacts'
        required: false
        type: string

jobs:
  publish-s3:
    name: Publish docs to S3 and flush Akamai
    runs-on: ubuntu-latest
    # Only run if the generate-docs workflow succeeded or this was manually triggered
    if: ${{ github.event_name != 'workflow_run' || github.event.workflow_run.conclusion == 'success' }}

    permissions:
      contents: read
      id-token: write
      actions: read

    env:
      S3_BUCKET: s3://brightspot-assets-prod/developer/docs
      DOCS_DIR: dynamo-docs

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          # Use OIDC (role assumption) if available, otherwise use IAM keys
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-access-key-id: ${{ secrets.AWS_ROLE_TO_ASSUME && '' || secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_ROLE_TO_ASSUME && '' || secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION || 'us-east-1' }}

      - name: Verify AWS identity
        run: |
          aws sts get-caller-identity >/dev/null || {
            echo "::error::Failed to authenticate with AWS. Check credentials configuration."
            exit 1
          }

      - name: Download documentation artifacts
        id: download
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Determine which run to get artifacts from
            let runId = context.payload.workflow_run?.id ||
                       context.payload.inputs?.run_id;

            if (context.eventName === 'workflow_run') {
              // Triggered by generate-docs workflow completion
              runId = context.payload.workflow_run.id;
              console.log(`Using artifacts from triggering workflow run: ${runId}`);
            } else if (context.eventName === 'workflow_dispatch' && context.payload.inputs?.run_id) {
              // Manual trigger with specific run ID
              runId = context.payload.inputs.run_id;
              console.log(`Using artifacts from specified run: ${runId}`);
            } else {
              // For tag pushes or manual trigger without run_id, find the latest successful generate-docs run
              const runs = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: 'generate-docs.yml',
                branch: 'main',
                status: 'success',
                per_page: 1
              });

              if (runs.data.workflow_runs.length === 0) {
                throw new Error('No successful generate-docs workflow runs found');
              }
              runId = runs.data.workflow_runs[0].id;
            }

            console.log(`Using artifacts from run: ${runId}`);

            // Get and download the docs artifact
            const { data: artifacts } = await github.rest.actions.listWorkflowRunArtifacts({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id: runId
            });

            const artifact = artifacts.artifacts.find(a => a.name.startsWith('dynamo-docs-'));
            if (!artifact) {
              throw new Error(`No documentation artifact found in run ${runId}`);
            }

            console.log(`Found artifact: ${artifact.name} (${artifact.size_in_bytes} bytes)`);

            const { data } = await github.rest.actions.downloadArtifact({
              owner: context.repo.owner,
              repo: context.repo.repo,
              artifact_id: artifact.id,
              archive_format: 'zip'
            });

            fs.writeFileSync('/tmp/artifact.zip', Buffer.from(data));
            core.setOutput('artifact_name', artifact.name);
            core.setOutput('run_id', runId);
            return `Downloaded ${artifact.name}`;

      - name: Extract documentation artifacts
        run: |
          echo "::notice::Extracting documentation from artifact: ${{ steps.download.outputs.artifact_name }}"
          mkdir -p ${{ env.DOCS_DIR }}
          unzip -q /tmp/artifact.zip -d ${{ env.DOCS_DIR }}

          # Validate extraction
          if [[ ! -d "${{ env.DOCS_DIR }}" ]] || [[ -z "$(ls -A ${{ env.DOCS_DIR }})" ]]; then
            echo "::error::Documentation directory is empty after extraction"
            exit 1
          fi

          echo "::notice::Documentation size: $(du -sh ${{ env.DOCS_DIR }} | cut -f1)"

      - name: Determine version and validate inputs
        id: vars
        env:
          ARTIFACTS_PATH: dynamo-docs
          TARGET_PATH: ${{ vars.PUBLISH_S3_TARGET_PATH }}
        shell: bash
        run: |
          set -euo pipefail

          if [[ -z "${TARGET_PATH}" ]]; then
            echo "::error::target-path was not provided. Set repository variable PUBLISH_S3_TARGET_PATH."
            exit 1
          fi

          if [[ ! -d "${ARTIFACTS_PATH}" ]]; then
            echo "::error::Failed to find documentation artifacts at ${ARTIFACTS_PATH}"
            exit 1
          fi

          # Parse version from tag if present
          VERSION=""
          if [[ "${{ github.ref_type }}" == "tag" ]]; then
            TAG="${{ github.ref_name }}"
            # Extract semantic version from tags like v1.2.3 or project-v1.2.3.post0
            if [[ "${TAG}" =~ v([0-9]+(\.[0-9]+){1,2}([._-](post|rc|dev)[0-9]+)?) ]]; then
              VERSION="${BASH_REMATCH[1]}"
            fi
          fi

          echo "version=${VERSION}" >> "$GITHUB_OUTPUT"
          echo "artifacts_path=${ARTIFACTS_PATH}" >> "$GITHUB_OUTPUT"

      - name: Normalize S3 path
        id: paths
        env:
          S3_TARGET_ROOT: ${{ env.S3_BUCKET }}
          TARGET_PATH: ${{ vars.PUBLISH_S3_TARGET_PATH }}
        shell: bash
        run: |
          set -euo pipefail
          S3_ROOT="${S3_TARGET_ROOT%/}"
          S3_PATH="${TARGET_PATH#/}"
          S3_PATH="${S3_PATH%/}"
          echo "S3_TARGET_PATH...${S3_PATH}"
          echo "s3_root=${S3_ROOT}" >> "$GITHUB_OUTPUT"
          echo "s3_path=${S3_PATH}" >> "$GITHUB_OUTPUT"

      - name: Publish version (if tagged)
        if: ${{ steps.vars.outputs.version != '' }}
        working-directory: ${{ env.DOCS_DIR }}
        env:
          S3_ROOT: ${{ steps.paths.outputs.s3_root }}
          S3_PATH: ${{ steps.paths.outputs.s3_path }}
          VERSION: ${{ steps.vars.outputs.version }}
        shell: bash
        run: |
          set -euo pipefail
          echo "Publishing version ${VERSION} to ${S3_ROOT}/${S3_PATH}/${VERSION}"
          aws s3 sync . "${S3_ROOT}/${S3_PATH}/${VERSION}" --exclude .buildinfo --exclude .doctrees --delete
          # Copy version manifest files if they exist
          for file in versions.json versions1.json; do
            if [[ -f "${file}" ]]; then
              echo "Copying ${file} to ${S3_ROOT}/${S3_PATH}/"
              aws s3 cp "${file}" "${S3_ROOT}/${S3_PATH}/" || {
                echo "::warning::Failed to copy ${file} to S3"
              }
            fi
          done

      - name: Publish latest
        # Skip if commit message contains '/not-latest' anywhere
        if: ${{ startsWith(github.ref, 'refs/heads/main') && !contains(github.event.head_commit.message, '/not-latest') }}
        working-directory: ${{ env.DOCS_DIR }}
        id: publish_latest
        env:
          S3_ROOT: ${{ steps.paths.outputs.s3_root }}
          S3_PATH: ${{ steps.paths.outputs.s3_path }}
        shell: bash
        run: |
          set -euo pipefail
          echo "Publishing latest to ${S3_ROOT}/${S3_PATH}/latest"
          aws s3 sync . "${S3_ROOT}/${S3_PATH}/latest" --exclude .buildinfo --exclude .doctrees --delete
          echo "published=true" >> "$GITHUB_OUTPUT"

      - name: Collect publish outputs
        id: publish
        env:
          S3_PATH: ${{ steps.paths.outputs.s3_path }}
          VERSION: ${{ steps.vars.outputs.version }}
          PUBLISHED_LATEST: ${{ steps.publish_latest.outputs.published || 'false' }}
        shell: bash
        run: |
          set -euo pipefail
          echo "s3_target_path=${S3_PATH}" >> "$GITHUB_OUTPUT"
          echo "request_name=Publish docs from ${GITHUB_REPOSITORY}@${GITHUB_SHA:0:8}" >> "$GITHUB_OUTPUT"

          # Only flush cache if we actually published something
          if [[ -n "${VERSION}" ]] || [[ "${PUBLISHED_LATEST}" == "true" ]]; then
            echo "perform_flush=true" >> "$GITHUB_OUTPUT"
          else
            echo "perform_flush=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Flush Akamai cache
        # Only run if cache flush is needed AND Akamai is enabled
        if: ${{ steps.publish.outputs.perform_flush == 'true' && vars.AKAMAI_ENABLED == 'true' }}
        env:
          S3_PATH: ${{ steps.publish.outputs.s3_target_path }}
          REQUEST_NAME: ${{ steps.publish.outputs.request_name }}
          # Use repository variable or secret for notification emails
          # Format: JSON array of email addresses, e.g., '["email1@example.com", "email2@example.com"]'
          EMAILS_JSON: ${{ vars.AKAMAI_NOTIFICATION_EMAILS || secrets.AKAMAI_NOTIFICATION_EMAILS || '["devops@nvidia.com"]' }}
          AKAMAI_CLIENT_SECRET: ${{ secrets.AKAMAI_CLIENT_SECRET }}
          AKAMAI_HOST: ${{ secrets.AKAMAI_HOST }}
          AKAMAI_ACCESS_TOKEN: ${{ secrets.AKAMAI_ACCESS_TOKEN }}
          AKAMAI_CLIENT_TOKEN: ${{ secrets.AKAMAI_CLIENT_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail

          # Install required tools for Akamai
          sudo apt-get update -qq
          sudo apt-get install -y -qq jq xsltproc
          pip install -q httpie httpie-edgegrid

          # Generate Akamai ECCU request XML using the XSLT template
          XSLT_TEMPLATE="${GITHUB_WORKSPACE}/.github/workflows/templates/akamai-eccu-flush.xslt"

          if [[ ! -f "${XSLT_TEMPLATE}" ]]; then
            echo "::error::XSLT template file not found at ${XSLT_TEMPLATE}"
            exit 1
          fi

          # Process XSLT to generate ECCU request XML
          xsltproc --stringparam target-path "${S3_PATH}" "${XSLT_TEMPLATE}" "${XSLT_TEMPLATE}" | \
            sed 's/xmlns:match="x" //' > /tmp/flush.xml

          # Prepare Akamai EdgeGrid credentials
          echo "[default]"                                > ~/.edgerc
          echo "client_secret = ${AKAMAI_CLIENT_SECRET}" >> ~/.edgerc
          echo "host = ${AKAMAI_HOST}"                   >> ~/.edgerc
          echo "access_token = ${AKAMAI_ACCESS_TOKEN}"   >> ~/.edgerc
          echo "client_token = ${AKAMAI_CLIENT_TOKEN}"   >> ~/.edgerc

          # Validate and prepare email list JSON
          if [[ -n "${EMAILS_JSON}" ]]; then
            echo "${EMAILS_JSON}" | jq -c . > /tmp/email-addresses.json || {
              echo "::error::Invalid JSON format for AKAMAI_NOTIFICATION_EMAILS"
              exit 1
            }
          else
            echo '[]' > /tmp/email-addresses.json
          fi

          # Submit ECCU request to Akamai
          http --ignore-stdin --auth-type edgegrid -a default: POST :/eccu-api/v1/requests \
            metadata=@"/tmp/flush.xml" \
            propertyName=docs.nvidia.com \
            propertyNameExactMatch=true \
            propertyType=HOST_HEADER \
            requestName="${REQUEST_NAME}" \
            statusUpdateEmails:=@/tmp/email-addresses.json || {
              echo "::warning::Failed to flush Akamai cache, but continuing workflow"
              # Don't fail the workflow if cache flush fails
            }

      - name: Summary
        if: always()
        env:
          VERSION: ${{ steps.vars.outputs.version }}
          S3_PATH: ${{ steps.paths.outputs.s3_path }}
          ARTIFACT_NAME: ${{ steps.download.outputs.artifact_name }}
          RUN_ID: ${{ steps.download.outputs.run_id }}
          PUBLISHED_LATEST: ${{ steps.publish_latest.outputs.published || 'false' }}
          CACHE_FLUSHED: ${{ steps.publish.outputs.perform_flush }}
        run: |
          echo "## ðŸ“š Documentation Publishing Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Source" >> $GITHUB_STEP_SUMMARY
          echo "- **Artifact:** \`${ARTIFACT_NAME}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **From Run:** [#${RUN_ID}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${RUN_ID})" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Published To" >> $GITHUB_STEP_SUMMARY
          if [[ -n "${VERSION}" ]]; then
            echo "- âœ… **Version:** \`${VERSION}\` â†’ \`s3://brightspot-assets-prod/developer/docs/${S3_PATH}/${VERSION}\`" >> $GITHUB_STEP_SUMMARY
          fi
          if [[ "${PUBLISHED_LATEST}" == "true" ]]; then
            echo "- âœ… **Latest:** \`s3://brightspot-assets-prod/developer/docs/${S3_PATH}/latest\`" >> $GITHUB_STEP_SUMMARY
          fi
          if [[ -z "${VERSION}" ]] && [[ "${PUBLISHED_LATEST}" != "true" ]]; then
            echo "- âš ï¸ No documentation was published (no version tag or latest update)" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Cache" >> $GITHUB_STEP_SUMMARY
          if [[ "${CACHE_FLUSHED}" == "true" ]]; then
            echo "- âœ… Akamai cache flush requested" >> $GITHUB_STEP_SUMMARY
          else
            echo "- â­ï¸ Cache flush skipped (nothing published or Akamai disabled)" >> $GITHUB_STEP_SUMMARY
          fi